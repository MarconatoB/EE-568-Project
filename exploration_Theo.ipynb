{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0YeluhjU49M"
      },
      "outputs": [],
      "source": [
        "# https://github.com/haarnoja/sac\n",
        "import argparse\n",
        "import datetime\n",
        "import gym\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torch\n",
        "from sac import SAC\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from replay_memory import ReplayMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCcy8l-uU49O",
        "outputId": "d33183d1-3efb-417d-b225-b45053b2f953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class arguments():\n",
        "    def __init__(self):\n",
        "        self.env_name = \"MountainCarContinuous-v0\"\n",
        "        self.policy = \"Gaussian\"\n",
        "        self.gamma = 0.99\n",
        "        self.tau=0.005\n",
        "        self.lr = 0.0003\n",
        "        self.alpha = 0.2\n",
        "        self.automatic_entropy_tuning = False\n",
        "        self.seed=123456\n",
        "        self.batch_size=64\n",
        "        self.num_steps=500000\n",
        "        self.hidden_size=256\n",
        "        self.updates_per_step=1\n",
        "        self.start_steps=30000\n",
        "        self.target_update_interval=1\n",
        "        self.replay_size = 500000\n",
        "        self.cuda = False\n",
        "        self.eval = True\n",
        "\n",
        "\n",
        "args = arguments()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzhipubzU49O",
        "outputId": "e4ef7ae3-bef1-4b50-9c7a-8e5e951e8e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(args.env_name)\n",
        "env.reset(seed = args.seed)\n",
        "env.action_space.seed(args.seed)\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXXCuJ4fU49O",
        "outputId": "76a8b372-4a17-4cab-c419-77dfac142250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2730275]\n",
            "[-0.5230377  0.       ]\n"
          ]
        }
      ],
      "source": [
        "print(env.action_space.sample())\n",
        "print(env.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzZzZQ86U49O",
        "outputId": "de341e8c-016e-4925-dcf2-d25d164a9c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_inputs: 2\n",
            "action_space.shape[0]: (1,)\n",
            "args.hidden_size: 256\n"
          ]
        }
      ],
      "source": [
        "agent = SAC(env.observation_space.shape[0], env.action_space, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zviEjv64U49O",
        "outputId": "33a3887e-e8b5-4e67-8578-02dd76ac1615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "#Tensorboard\n",
        "writer = SummaryWriter('runs/{}_SAC_{}_{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), args.env_name,\n",
        "                                                             args.policy, \"autotune\" if args.automatic_entropy_tuning else \"\"))\n",
        "\n",
        "# Memory\n",
        "memory = ReplayMemory(args.replay_size, args.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbrUkxiwU49P",
        "outputId": "4e9390f9-2327-44fc-af97-8372d8ce3e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "999\n"
          ]
        }
      ],
      "source": [
        "print(env._max_episode_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_96y0IkU49P",
        "outputId": "3b0aa77b-97d5-4da7-c3fc-3a19c11b2d9a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1\n",
            "Episode: 1, total numsteps: 999, episode steps: 999, reward: -33.16\n",
            "episode: 2\n",
            "Episode: 2, total numsteps: 1998, episode steps: 999, reward: -32.78\n",
            "episode: 3\n",
            "Episode: 3, total numsteps: 2997, episode steps: 999, reward: -35.29\n",
            "episode: 4\n",
            "Episode: 4, total numsteps: 3996, episode steps: 999, reward: -32.28\n",
            "episode: 5\n",
            "Episode: 5, total numsteps: 4995, episode steps: 999, reward: -33.41\n",
            "episode: 6\n",
            "Episode: 6, total numsteps: 5994, episode steps: 999, reward: -32.45\n",
            "episode: 7\n",
            "Episode: 7, total numsteps: 6993, episode steps: 999, reward: -33.84\n",
            "episode: 8\n",
            "Episode: 8, total numsteps: 7992, episode steps: 999, reward: -33.35\n",
            "episode: 9\n",
            "Episode: 9, total numsteps: 8991, episode steps: 999, reward: -33.21\n",
            "episode: 10\n",
            "Episode: 10, total numsteps: 9990, episode steps: 999, reward: -33.0\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 11\n",
            "Episode: 11, total numsteps: 10989, episode steps: 999, reward: -34.89\n",
            "episode: 12\n",
            "Episode: 12, total numsteps: 11988, episode steps: 999, reward: -35.21\n",
            "episode: 13\n",
            "Episode: 13, total numsteps: 12987, episode steps: 999, reward: -33.31\n",
            "episode: 14\n",
            "Episode: 14, total numsteps: 13986, episode steps: 999, reward: -33.09\n",
            "episode: 15\n",
            "Episode: 15, total numsteps: 14985, episode steps: 999, reward: -32.97\n",
            "episode: 16\n",
            "Episode: 16, total numsteps: 15984, episode steps: 999, reward: -33.13\n",
            "episode: 17\n",
            "Episode: 17, total numsteps: 16983, episode steps: 999, reward: -32.59\n",
            "episode: 18\n",
            "Episode: 18, total numsteps: 17982, episode steps: 999, reward: -33.25\n",
            "episode: 19\n",
            "Episode: 19, total numsteps: 18981, episode steps: 999, reward: -35.69\n",
            "episode: 20\n",
            "Episode: 20, total numsteps: 19980, episode steps: 999, reward: -33.62\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.01\n",
            "----------------------------------------\n",
            "episode: 21\n",
            "Episode: 21, total numsteps: 20979, episode steps: 999, reward: -31.38\n",
            "episode: 22\n",
            "Episode: 22, total numsteps: 21978, episode steps: 999, reward: -33.63\n",
            "episode: 23\n",
            "Episode: 23, total numsteps: 22977, episode steps: 999, reward: -32.99\n",
            "episode: 24\n",
            "Episode: 24, total numsteps: 23976, episode steps: 999, reward: -32.68\n",
            "episode: 25\n",
            "Episode: 25, total numsteps: 24975, episode steps: 999, reward: -34.39\n",
            "episode: 26\n",
            "Episode: 26, total numsteps: 25974, episode steps: 999, reward: -33.07\n",
            "episode: 27\n",
            "Episode: 27, total numsteps: 26973, episode steps: 999, reward: -33.1\n",
            "episode: 28\n",
            "Episode: 28, total numsteps: 27972, episode steps: 999, reward: -31.62\n",
            "episode: 29\n",
            "Episode: 29, total numsteps: 28971, episode steps: 999, reward: -33.2\n",
            "episode: 30\n",
            "Episode: 30, total numsteps: 29970, episode steps: 999, reward: -33.98\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.02\n",
            "----------------------------------------\n",
            "episode: 31\n",
            "Episode: 31, total numsteps: 30969, episode steps: 999, reward: -31.18\n",
            "episode: 32\n",
            "Episode: 32, total numsteps: 31968, episode steps: 999, reward: -31.2\n",
            "episode: 33\n",
            "Episode: 33, total numsteps: 32967, episode steps: 999, reward: -29.38\n",
            "episode: 34\n",
            "Episode: 34, total numsteps: 33966, episode steps: 999, reward: -29.67\n",
            "episode: 35\n",
            "Episode: 35, total numsteps: 34965, episode steps: 999, reward: -30.85\n",
            "episode: 36\n",
            "Episode: 36, total numsteps: 35964, episode steps: 999, reward: -30.45\n",
            "episode: 37\n",
            "Episode: 37, total numsteps: 36963, episode steps: 999, reward: -31.28\n",
            "episode: 38\n",
            "Episode: 38, total numsteps: 37962, episode steps: 999, reward: -30.42\n",
            "episode: 39\n",
            "Episode: 39, total numsteps: 38961, episode steps: 999, reward: -30.62\n",
            "episode: 40\n",
            "Episode: 40, total numsteps: 39960, episode steps: 999, reward: -30.57\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 41\n",
            "Episode: 41, total numsteps: 40959, episode steps: 999, reward: -29.57\n",
            "episode: 42\n",
            "Episode: 42, total numsteps: 41958, episode steps: 999, reward: -29.58\n",
            "episode: 43\n",
            "Episode: 43, total numsteps: 42957, episode steps: 999, reward: -30.48\n",
            "episode: 44\n",
            "Episode: 44, total numsteps: 43956, episode steps: 999, reward: -28.75\n",
            "episode: 45\n",
            "Episode: 45, total numsteps: 44955, episode steps: 999, reward: -30.52\n",
            "episode: 46\n",
            "Episode: 46, total numsteps: 45954, episode steps: 999, reward: -30.54\n",
            "episode: 47\n",
            "Episode: 47, total numsteps: 46953, episode steps: 999, reward: -31.04\n",
            "episode: 48\n",
            "Episode: 48, total numsteps: 47952, episode steps: 999, reward: -30.16\n",
            "episode: 49\n",
            "Episode: 49, total numsteps: 48951, episode steps: 999, reward: -30.74\n",
            "episode: 50\n",
            "Episode: 50, total numsteps: 49950, episode steps: 999, reward: -29.67\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.01\n",
            "----------------------------------------\n",
            "episode: 51\n",
            "Episode: 51, total numsteps: 50949, episode steps: 999, reward: -28.96\n",
            "episode: 52\n",
            "Episode: 52, total numsteps: 51948, episode steps: 999, reward: -30.58\n",
            "episode: 53\n",
            "Episode: 53, total numsteps: 52947, episode steps: 999, reward: -31.18\n",
            "episode: 54\n",
            "Episode: 54, total numsteps: 53946, episode steps: 999, reward: -30.18\n",
            "episode: 55\n",
            "Episode: 55, total numsteps: 54945, episode steps: 999, reward: -29.91\n",
            "episode: 56\n",
            "Episode: 56, total numsteps: 55944, episode steps: 999, reward: -29.33\n",
            "episode: 57\n",
            "Episode: 57, total numsteps: 56943, episode steps: 999, reward: -30.49\n",
            "episode: 58\n",
            "Episode: 58, total numsteps: 57942, episode steps: 999, reward: -30.67\n",
            "episode: 59\n",
            "Episode: 59, total numsteps: 58941, episode steps: 999, reward: -28.61\n",
            "episode: 60\n",
            "Episode: 60, total numsteps: 59940, episode steps: 999, reward: -30.0\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 61\n",
            "Episode: 61, total numsteps: 60939, episode steps: 999, reward: -30.44\n",
            "episode: 62\n",
            "Episode: 62, total numsteps: 61938, episode steps: 999, reward: -31.35\n",
            "episode: 63\n",
            "Episode: 63, total numsteps: 62937, episode steps: 999, reward: -31.06\n",
            "episode: 64\n",
            "Episode: 64, total numsteps: 63936, episode steps: 999, reward: -30.25\n",
            "episode: 65\n",
            "Episode: 65, total numsteps: 64935, episode steps: 999, reward: -29.4\n",
            "episode: 66\n",
            "Episode: 66, total numsteps: 65934, episode steps: 999, reward: -31.63\n",
            "episode: 67\n",
            "Episode: 67, total numsteps: 66933, episode steps: 999, reward: -30.38\n",
            "episode: 68\n",
            "Episode: 68, total numsteps: 67932, episode steps: 999, reward: -31.7\n",
            "episode: 69\n",
            "Episode: 69, total numsteps: 68931, episode steps: 999, reward: -30.45\n",
            "episode: 70\n",
            "Episode: 70, total numsteps: 69930, episode steps: 999, reward: -29.84\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 71\n",
            "Episode: 71, total numsteps: 70929, episode steps: 999, reward: -31.55\n",
            "episode: 72\n",
            "Episode: 72, total numsteps: 71928, episode steps: 999, reward: -31.03\n",
            "episode: 73\n",
            "Episode: 73, total numsteps: 72927, episode steps: 999, reward: -29.74\n",
            "episode: 74\n",
            "Episode: 74, total numsteps: 73926, episode steps: 999, reward: -29.21\n",
            "episode: 75\n",
            "Episode: 75, total numsteps: 74925, episode steps: 999, reward: -30.71\n",
            "episode: 76\n",
            "Episode: 76, total numsteps: 75924, episode steps: 999, reward: -31.21\n",
            "episode: 77\n",
            "Episode: 77, total numsteps: 76923, episode steps: 999, reward: -30.56\n",
            "episode: 78\n",
            "Episode: 78, total numsteps: 77922, episode steps: 999, reward: -30.87\n",
            "episode: 79\n",
            "Episode: 79, total numsteps: 78921, episode steps: 999, reward: -31.55\n",
            "episode: 80\n",
            "Episode: 80, total numsteps: 79920, episode steps: 999, reward: -30.38\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.01\n",
            "----------------------------------------\n",
            "episode: 81\n",
            "Episode: 81, total numsteps: 80919, episode steps: 999, reward: -31.74\n",
            "episode: 82\n",
            "Episode: 82, total numsteps: 81918, episode steps: 999, reward: -28.91\n",
            "episode: 83\n",
            "Episode: 83, total numsteps: 82917, episode steps: 999, reward: -31.13\n",
            "episode: 84\n",
            "Episode: 84, total numsteps: 83916, episode steps: 999, reward: -32.17\n",
            "episode: 85\n",
            "Episode: 85, total numsteps: 84915, episode steps: 999, reward: -30.24\n",
            "episode: 86\n",
            "Episode: 86, total numsteps: 85914, episode steps: 999, reward: -30.35\n",
            "episode: 87\n",
            "Episode: 87, total numsteps: 86913, episode steps: 999, reward: -30.82\n",
            "episode: 88\n",
            "Episode: 88, total numsteps: 87912, episode steps: 999, reward: -28.74\n",
            "episode: 89\n",
            "Episode: 89, total numsteps: 88911, episode steps: 999, reward: -31.75\n",
            "episode: 90\n",
            "Episode: 90, total numsteps: 89910, episode steps: 999, reward: -30.23\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.01\n",
            "----------------------------------------\n",
            "episode: 91\n",
            "Episode: 91, total numsteps: 90909, episode steps: 999, reward: -30.1\n",
            "episode: 92\n",
            "Episode: 92, total numsteps: 91908, episode steps: 999, reward: -30.56\n",
            "episode: 93\n",
            "Episode: 93, total numsteps: 92907, episode steps: 999, reward: -31.02\n",
            "episode: 94\n",
            "Episode: 94, total numsteps: 93906, episode steps: 999, reward: -31.24\n",
            "episode: 95\n",
            "Episode: 95, total numsteps: 94905, episode steps: 999, reward: -30.48\n",
            "episode: 96\n",
            "Episode: 96, total numsteps: 95904, episode steps: 999, reward: -30.2\n",
            "episode: 97\n",
            "Episode: 97, total numsteps: 96903, episode steps: 999, reward: -30.28\n",
            "episode: 98\n",
            "Episode: 98, total numsteps: 97902, episode steps: 999, reward: -30.17\n",
            "episode: 99\n",
            "Episode: 99, total numsteps: 98901, episode steps: 999, reward: -30.79\n",
            "episode: 100\n",
            "Episode: 100, total numsteps: 99900, episode steps: 999, reward: -28.76\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 101\n",
            "Episode: 101, total numsteps: 100899, episode steps: 999, reward: -29.27\n",
            "episode: 102\n",
            "Episode: 102, total numsteps: 101898, episode steps: 999, reward: -30.37\n",
            "episode: 103\n",
            "Episode: 103, total numsteps: 102897, episode steps: 999, reward: -30.84\n",
            "episode: 104\n",
            "Episode: 104, total numsteps: 103896, episode steps: 999, reward: -31.27\n",
            "episode: 105\n",
            "Episode: 105, total numsteps: 104895, episode steps: 999, reward: -28.81\n",
            "episode: 106\n",
            "Episode: 106, total numsteps: 105894, episode steps: 999, reward: -29.74\n",
            "episode: 107\n",
            "Episode: 107, total numsteps: 106893, episode steps: 999, reward: -29.58\n",
            "episode: 108\n",
            "Episode: 108, total numsteps: 107892, episode steps: 999, reward: -30.1\n",
            "episode: 109\n",
            "Episode: 109, total numsteps: 108891, episode steps: 999, reward: -30.07\n",
            "episode: 110\n",
            "Episode: 110, total numsteps: 109890, episode steps: 999, reward: -29.92\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 111\n",
            "Episode: 111, total numsteps: 110889, episode steps: 999, reward: -30.32\n",
            "episode: 112\n",
            "Episode: 112, total numsteps: 111888, episode steps: 999, reward: -30.44\n",
            "episode: 113\n",
            "Episode: 113, total numsteps: 112887, episode steps: 999, reward: -30.76\n",
            "episode: 114\n",
            "Episode: 114, total numsteps: 113886, episode steps: 999, reward: -31.68\n",
            "episode: 115\n",
            "Episode: 115, total numsteps: 114885, episode steps: 999, reward: -31.49\n",
            "episode: 116\n",
            "Episode: 116, total numsteps: 115884, episode steps: 999, reward: -29.89\n",
            "episode: 117\n",
            "Episode: 117, total numsteps: 116883, episode steps: 999, reward: -29.65\n",
            "episode: 118\n",
            "Episode: 118, total numsteps: 117882, episode steps: 999, reward: -29.4\n",
            "episode: 119\n",
            "Episode: 119, total numsteps: 118881, episode steps: 999, reward: -30.75\n",
            "episode: 120\n",
            "Episode: 120, total numsteps: 119880, episode steps: 999, reward: -32.15\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 121\n",
            "Episode: 121, total numsteps: 120879, episode steps: 999, reward: -30.79\n",
            "episode: 122\n",
            "Episode: 122, total numsteps: 121878, episode steps: 999, reward: -31.17\n",
            "episode: 123\n",
            "Episode: 123, total numsteps: 122877, episode steps: 999, reward: -28.93\n",
            "episode: 124\n",
            "Episode: 124, total numsteps: 123876, episode steps: 999, reward: -29.7\n",
            "episode: 125\n",
            "Episode: 125, total numsteps: 124875, episode steps: 999, reward: -29.12\n",
            "episode: 126\n",
            "Episode: 126, total numsteps: 125874, episode steps: 999, reward: -30.03\n",
            "episode: 127\n",
            "Episode: 127, total numsteps: 126873, episode steps: 999, reward: -30.01\n",
            "episode: 128\n",
            "Episode: 128, total numsteps: 127872, episode steps: 999, reward: -31.26\n",
            "episode: 129\n",
            "Episode: 129, total numsteps: 128871, episode steps: 999, reward: -29.58\n",
            "episode: 130\n",
            "Episode: 130, total numsteps: 129870, episode steps: 999, reward: -29.83\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 131\n",
            "Episode: 131, total numsteps: 130869, episode steps: 999, reward: -32.13\n",
            "episode: 132\n",
            "Episode: 132, total numsteps: 131868, episode steps: 999, reward: -28.52\n",
            "episode: 133\n",
            "Episode: 133, total numsteps: 132867, episode steps: 999, reward: -28.88\n",
            "episode: 134\n",
            "Episode: 134, total numsteps: 133866, episode steps: 999, reward: -31.28\n",
            "episode: 135\n",
            "Episode: 135, total numsteps: 134865, episode steps: 999, reward: -30.6\n",
            "episode: 136\n",
            "Episode: 136, total numsteps: 135864, episode steps: 999, reward: -30.29\n",
            "episode: 137\n",
            "Episode: 137, total numsteps: 136863, episode steps: 999, reward: -28.96\n",
            "episode: 138\n",
            "Episode: 138, total numsteps: 137862, episode steps: 999, reward: -30.66\n",
            "episode: 139\n",
            "Episode: 139, total numsteps: 138861, episode steps: 999, reward: -30.42\n",
            "episode: 140\n",
            "Episode: 140, total numsteps: 139860, episode steps: 999, reward: -29.17\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.01\n",
            "----------------------------------------\n",
            "episode: 141\n",
            "Episode: 141, total numsteps: 140859, episode steps: 999, reward: -29.81\n",
            "episode: 142\n",
            "Episode: 142, total numsteps: 141858, episode steps: 999, reward: -28.7\n",
            "episode: 143\n",
            "Episode: 143, total numsteps: 142857, episode steps: 999, reward: -31.2\n",
            "episode: 144\n",
            "Episode: 144, total numsteps: 143856, episode steps: 999, reward: -30.01\n",
            "episode: 145\n",
            "Episode: 145, total numsteps: 144855, episode steps: 999, reward: -29.48\n",
            "episode: 146\n",
            "Episode: 146, total numsteps: 145854, episode steps: 999, reward: -31.02\n",
            "episode: 147\n",
            "Episode: 147, total numsteps: 146853, episode steps: 999, reward: -30.4\n",
            "episode: 148\n",
            "Episode: 148, total numsteps: 147852, episode steps: 999, reward: -30.39\n",
            "episode: 149\n",
            "Episode: 149, total numsteps: 148851, episode steps: 999, reward: -30.99\n",
            "episode: 150\n",
            "Episode: 150, total numsteps: 149850, episode steps: 999, reward: -30.3\n",
            "----------------------------------------\n",
            "Test Episodes: 10, Avg. Reward: -0.0\n",
            "----------------------------------------\n",
            "episode: 151\n",
            "Episode: 151, total numsteps: 150849, episode steps: 999, reward: -29.92\n",
            "episode: 152\n",
            "Episode: 152, total numsteps: 151848, episode steps: 999, reward: -31.11\n",
            "episode: 153\n",
            "Episode: 153, total numsteps: 152847, episode steps: 999, reward: -29.0\n",
            "episode: 154\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "total_numsteps = 0\n",
        "updates = 0\n",
        "\n",
        "for i_episode in itertools.count(1):\n",
        "    print(f'episode: {i_episode}')\n",
        "    episode_reward = 0\n",
        "    episode_steps = 0\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        #print(f'total_numsteps: {total_numsteps}')\n",
        "        #print(f'episode_steps: {episode_steps}')\n",
        "        if args.start_steps > total_numsteps:\n",
        "            action = env.action_space.sample()  # Sample random action\n",
        "        else:\n",
        "            #print(\"policy following\")\n",
        "            action = agent.select_action(state)  # Sample action from policy\n",
        "\n",
        "        if len(memory) > args.batch_size:\n",
        "            # Number of updates per step in environment\n",
        "            for i in range(args.updates_per_step):\n",
        "                # Update parameters of all the networks\n",
        "                critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, args.batch_size, updates)\n",
        "\n",
        "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
        "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
        "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
        "                writer.add_scalar('loss/entropy_loss', ent_loss, updates)\n",
        "                writer.add_scalar('entropy_temprature/alpha', alpha, updates)\n",
        "                updates += 1\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action) # Step\n",
        "        episode_steps += 1\n",
        "        total_numsteps += 1\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
        "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
        "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
        "\n",
        "        memory.push(state, action, reward, next_state, mask) # Append transition to memory\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    if total_numsteps > args.num_steps:\n",
        "        break\n",
        "\n",
        "    #print('a')\n",
        "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
        "    print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {}\".format(i_episode, total_numsteps, episode_steps, round(episode_reward, 2)))\n",
        "    #print('b')\n",
        "\n",
        "    if i_episode % 10 == 0 and args.eval is True:\n",
        "        avg_reward = 0.\n",
        "        episodes = 10\n",
        "        for _  in range(episodes):\n",
        "            state = env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = agent.select_action(state, evaluate=True)\n",
        "\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                episode_reward += reward\n",
        "\n",
        "\n",
        "                state = next_state\n",
        "            avg_reward += episode_reward\n",
        "        avg_reward /= episodes\n",
        "\n",
        "\n",
        "        writer.add_scalar('avg_reward/test', avg_reward, i_episode)\n",
        "\n",
        "        print(\"----------------------------------------\")\n",
        "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
        "        print(\"----------------------------------------\")\n",
        "\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}