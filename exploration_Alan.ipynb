{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gym\n",
    "#!pip install --upgrade gym-notices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/Library/Python/3.9/lib/python/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cuda:0\" if torch.cuda.is_available() else\n",
    "                      \"cpu\")\n",
    "device\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "#env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy\n",
    "Unlike value-based method, the output of policy-based method is the probability of each action. It can be represented as policy. So activation function of output layer will be softmax, not ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        # we just consider 1 dimensional probability of action\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # we just consider 1 dimensional probability of action\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac(policy, optimizer, early_stop, n_episode):\n",
    "    state_old = env.reset()\n",
    "    buffer = []\n",
    "    action, _ = policy.act(state_old)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    buffer.append({\"s\" : state_old,\n",
    "                   \"a\" : action,\n",
    "                   \"r\" : reward,\n",
    "                   \"sprime\" : state,\n",
    "                   \"done\" : done})\n",
    "\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 22.0\n",
      "Episode Reward: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/6_49hd192z5c33kgxtpcm_040000gn/T/ipykernel_43614/2854446303.py:112: UserWarning: Using a target size (torch.Size([128, 2])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic1_loss = F.mse_loss(q1, expected_q_values)\n",
      "/var/folders/55/6_49hd192z5c33kgxtpcm_040000gn/T/ipykernel_43614/2854446303.py:113: UserWarning: Using a target size (torch.Size([128, 2])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic2_loss = F.mse_loss(q2, expected_q_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 39.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 18.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 22.0\n",
      "Episode Reward: 91.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 27.0\n",
      "Episode Reward: 23.0\n",
      "Episode Reward: 43.0\n",
      "Episode Reward: 57.0\n",
      "Episode Reward: 51.0\n",
      "Episode Reward: 49.0\n",
      "Episode Reward: 41.0\n",
      "Episode Reward: 36.0\n",
      "Episode Reward: 34.0\n",
      "Episode Reward: 26.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 20.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 34.0\n",
      "Episode Reward: 36.0\n",
      "Episode Reward: 35.0\n",
      "Episode Reward: 27.0\n",
      "Episode Reward: 45.0\n",
      "Episode Reward: 34.0\n",
      "Episode Reward: 22.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 26.0\n",
      "Episode Reward: 43.0\n",
      "Episode Reward: 42.0\n",
      "Episode Reward: 38.0\n",
      "Episode Reward: 34.0\n",
      "Episode Reward: 31.0\n",
      "Episode Reward: 27.0\n",
      "Episode Reward: 31.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 20.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 20.0\n",
      "Episode Reward: 23.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 21.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 23.0\n",
      "Episode Reward: 34.0\n",
      "Episode Reward: 25.0\n",
      "Episode Reward: 22.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 25.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 18.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 42.0\n",
      "Episode Reward: 50.0\n",
      "Episode Reward: 60.0\n",
      "Episode Reward: 51.0\n",
      "Episode Reward: 30.0\n",
      "Episode Reward: 41.0\n",
      "Episode Reward: 43.0\n",
      "Episode Reward: 18.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 20.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 21.0\n",
      "Episode Reward: 17.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 17.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 14.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 12.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 11.0\n",
      "Episode Reward: 13.0\n",
      "Episode Reward: 17.0\n",
      "Episode Reward: 15.0\n",
      "Episode Reward: 17.0\n",
      "Episode Reward: 24.0\n",
      "Episode Reward: 36.0\n",
      "Episode Reward: 32.0\n",
      "Episode Reward: 19.0\n",
      "Episode Reward: 20.0\n",
      "Episode Reward: 33.0\n",
      "Episode Reward: 23.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 16.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 8.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 9.0\n",
      "Episode Reward: 10.0\n",
      "Episode Reward: 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 203\u001b[0m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state)\u001b[38;5;66;03m#.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer\u001b[38;5;241m.\u001b[39mstorage) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m128\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m             \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode Reward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, episode_reward)\n\u001b[1;32m    207\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[31], line 128\u001b[0m, in \u001b[0;36mSACAgent.train\u001b[0;34m(self, replay_buffer, batch_size, gamma, alpha)\u001b[0m\n\u001b[1;32m    125\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (alpha \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(actions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_q_network(state_batch, actions))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "\n",
    "# Define the actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define the critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        #print(len(np.shape(action)))\n",
    "        #if len(np.shape(action)) == 1:\n",
    "        #    action = torch.FloatTensor(action).unsqueeze(1) # désolé pour ça\n",
    "        #print(\"action\", action)\n",
    "        x = torch.cat([state.squeeze(), action], dim=1)\n",
    "        #print(np.shape(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #print(\"survived\")\n",
    "        return x\n",
    "\n",
    "# Define the soft Q-function network\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the Soft Actor-Critic agent\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, lr=3e-4):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim)\n",
    "        self.critic1 = Critic(state_dim, action_dim, hidden_dim)\n",
    "        self.critic2 = Critic(state_dim, action_dim, hidden_dim)\n",
    "        self.target_critic1 = Critic(state_dim, action_dim, hidden_dim)\n",
    "        self.target_critic2 = Critic(state_dim, action_dim, hidden_dim)\n",
    "        self.soft_q_network = SoftQNetwork(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr)\n",
    "        self.soft_q_network_optimizer = optim.Adam(self.soft_q_network.parameters(), lr=lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        #print(state)\n",
    "        #state = torch.FloatTensor(state[0]).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        #print(action)\n",
    "        #print(action)\n",
    "        #print(action.cpu()[0])\n",
    "        #print(action.detach().numpy())\n",
    "        #action_probs = F.softmax(action.detach().numpy())  # Sample from softmax (discrete) or Gaussian (continuous)\n",
    "        action_probs = action.exp().detach().numpy()\n",
    "        #print(np.argmax(action_probs))\n",
    "        return action_probs\n",
    "    \n",
    "    def train(self, replay_buffer, batch_size=128, gamma=0.99, alpha=0.2):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        #print(\"batch\",action_batch)\n",
    "        state_batch = torch.FloatTensor(state_batch)\n",
    "        action_batch = torch.FloatTensor(action_batch)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "        done_batch = torch.FloatTensor(np.float32(done_batch)).unsqueeze(1)\n",
    "\n",
    "        # Update Q-networks\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor(next_state_batch)\n",
    "            #print(\"next\", next_actions)\n",
    "            next_q_values1 = self.target_critic1(next_state_batch, next_actions)\n",
    "            next_q_values2 = self.target_critic2(next_state_batch, next_actions)\n",
    "            next_q_values = torch.min(next_q_values1, next_q_values2) - alpha * torch.log(self.actor(next_state_batch))\n",
    "\n",
    "            expected_q_values = reward_batch + gamma * (1 - done_batch) * next_q_values\n",
    "        #print(\"torchtensor\",action_batch)\n",
    "        q1 = self.critic1(state_batch, action_batch)\n",
    "        q2 = self.critic2(state_batch, action_batch)\n",
    "\n",
    "        critic1_loss = F.mse_loss(q1, expected_q_values)\n",
    "        critic2_loss = F.mse_loss(q2, expected_q_values)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # Update actor\n",
    "        actions = self.actor(state_batch)\n",
    "        actor_loss = (alpha * torch.log(actions) - self.soft_q_network(state_batch, actions)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        for target_param, param in zip(self.target_critic1.parameters(), self.critic1.parameters()):\n",
    "            target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)\n",
    "\n",
    "        for target_param, param in zip(self.target_critic2.parameters(), self.critic2.parameters()):\n",
    "            target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] if isinstance(env.action_space, gym.spaces.Box) else env.action_space.n\n",
    "# Initialize agent\n",
    "agent = SACAgent(state_dim, action_dim)\n",
    "\n",
    "# Initialize replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        #print(transition[0])\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.ptr += 1\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.ptr, size=batch_size)\n",
    "        #print(len(self.storage),self.ptr,ind)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            s, a, r, s_, d = self.storage[i]\n",
    "            #print(\"s:\", s)\n",
    "            states.append(np.array(s, copy=False))\n",
    "            #print(states)\n",
    "            #print(np.array(states))\n",
    "            actions.append(np.array(a, copy=False))\n",
    "            #print(actions)\n",
    "            #print(np.array(actions))\n",
    "            rewards.append(np.array(r, copy=False))\n",
    "            #print(rewards)\n",
    "            #print(np.array(rewards))\n",
    "            next_states.append(np.array(s_, copy=False))\n",
    "            #print(next_states)\n",
    "            #print(np.array(next_states))\n",
    "            dones.append(np.array(d, copy=False))\n",
    "            #print(dones)\n",
    "            #print(np.array(dones))\n",
    "        #print(\"sample\",np.array(actions))\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "# Training loop\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "for _ in range(1000):  # Run for 1000 episodes\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state[0])#.unsqueeze(0)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        #print(action)\n",
    "        next_state, reward, done, _, _ = env.step(np.argmax(action))\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        episode_reward += reward\n",
    "        state = torch.FloatTensor(next_state)#.unsqueeze(0)\n",
    "\n",
    "        if len(replay_buffer.storage) > 128:\n",
    "            agent.train(replay_buffer)\n",
    "\n",
    "    print(\"Episode Reward:\", episode_reward)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
