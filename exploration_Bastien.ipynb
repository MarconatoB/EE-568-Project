{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/bastienmarconato/miniconda3/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/bastienmarconato/miniconda3/lib/python3.11/site-packages (from gymnasium) (1.26.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/bastienmarconato/miniconda3/lib/python3.11/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/bastienmarconato/miniconda3/lib/python3.11/site-packages (from gymnasium) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/bastienmarconato/miniconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gym-notices in /Users/bastienmarconato/miniconda3/lib/python3.11/site-packages (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gym-notices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import base64, io\n",
    "\n",
    "# For visualization\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from IPython.display import HTML\n",
    "from IPython import display\n",
    "import glob\n",
    "\n",
    "#device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                      #\"cuda:0\" if torch.cuda.is_available() else\n",
    "                      #\"cpu\")\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "* evaluation\n",
    "* Metal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acrobot-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:30<00:00, 323.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:29<00:00, 342.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MountainCarContinuous-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:31<00:00, 316.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MountainCar-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:29<00:00, 336.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pendulum-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:31<00:00, 318.55it/s] \n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = np.array([])\n",
    "\n",
    "    def add_entry(self, state, action, reward, next_state, done):\n",
    "        self.buffer = np.append(self.buffer, {\n",
    "            'state': torch.from_numpy(state),          # add batch dimension\n",
    "            'action': torch.tensor([action]),\n",
    "            'reward': torch.tensor([reward], dtype=torch.float32),\n",
    "            'next_state': torch.from_numpy(next_state),\n",
    "            'done': torch.tensor([done], dtype=torch.int)\n",
    "        })\n",
    "    \n",
    "    def sample(self, batch_size=1) -> dict:\n",
    "        indices = np.random.randint(len(self.buffer), size=batch_size)\n",
    "        batch = {\n",
    "            key: torch.stack([self.buffer[i][key] for i in indices], dim=0)\n",
    "            for key in self.buffer[0].keys()}\n",
    "        return batch\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        probs = F.relu(self.l1(state))\n",
    "        probs = F.relu(self.l2(probs))\n",
    "        probs = F.tanh(self.l3(probs))\n",
    "\n",
    "        return probs\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat([state, action], dim=1)\n",
    "\n",
    "        q = F.relu(self.l1(state_action))\n",
    "        q = F.relu(self.l2(q))\n",
    "        q = F.relu(self.l3(q))\n",
    "\n",
    "        return q\n",
    "    \n",
    "\n",
    "EXPL_NOISE = 0.1\n",
    "POLICY_NOISE = 0.2  # standard deviation of noise added to target policy during critic update\n",
    "NOISE_CLIP = 0.5    # clip target policy noise\n",
    "BATCH_SIZE = 256      # 256 \n",
    "DISCOUNT = 0.99\n",
    "TAU = 0.005         # target network update rate\n",
    "START_TIME = 25e2     # 25e3 in official implementation\n",
    "POLICY_FREQ = 2     # frequency of delayed policy updates\n",
    "\n",
    "def train_TD3(env, max_t=10000):    # 1e6 timesteps in official implementation\n",
    "\n",
    "    # all classical control envs have continuous states\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "    # check if action space is discrete\n",
    "    discrete_actions = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    if discrete_actions:\n",
    "        action_dim = 1\n",
    "        min_action = 0\n",
    "        max_action = env.action_space.n\n",
    "\n",
    "    else:   # continuous action space\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        assert env.action_space.low.shape == (1,), 'env has action_dim > 1'\n",
    "        min_action = env.action_space.low[0]\n",
    "        max_action = env.action_space.high[0]\n",
    "\n",
    "    actor = Actor(state_dim, action_dim).to(device)\n",
    "    target_actor = copy.deepcopy(actor)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "\n",
    "    critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "    critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "    target_critic_1 = copy.deepcopy(critic_1)\n",
    "    target_critic_2 = copy.deepcopy(critic_2)\n",
    "    critic_optimizer = torch.optim.Adam(\n",
    "        list(critic_1.parameters()) + list(critic_2.parameters()),\n",
    "        lr=3e-4)\n",
    "\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    state = env.reset()[0]\n",
    "    for step in tqdm(range(max_t)):     # TODO: handle episodes ??\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if step < START_TIME:\n",
    "                # Start by exploration\n",
    "                action = env.action_space.sample()  # outputs (1,) array in continuous envs\n",
    "            else:\n",
    "                # Select action with noise\n",
    "                noise = torch.normal(mean=0.0, std=EXPL_NOISE, size=(1,action_dim))\n",
    "                action = torch.clamp(\n",
    "                    actor(torch.from_numpy(state).squeeze()) + noise ,\n",
    "                    min=min_action, max=max_action\n",
    "                    )\n",
    "                if discrete_actions:    # Gymnasium expects a single value in this case\n",
    "                    action = int(action.item())\n",
    "                else:                   # and a (1,) array in continous envs\n",
    "                    action = action.flatten()\n",
    "                    \n",
    "            # Execute action and observe\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # for discrete/continuous compatibility purposes\n",
    "            if not discrete_actions:\n",
    "                action = action.item()\n",
    "\n",
    "            # Store tuple in replay buffer\n",
    "            replay_buffer.add_entry(state, action, reward, next_state, done)\n",
    "\n",
    "            # If s' is terminal, reset environment\n",
    "            if done:\n",
    "                state = env.reset()[0]\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        if step > START_TIME:\n",
    "            replay_batch = replay_buffer.sample(BATCH_SIZE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Select action with noise\n",
    "                noise = torch.normal(mean=0.0, std=EXPL_NOISE, size=(BATCH_SIZE, action_dim))\n",
    "                target_action = torch.clamp(\n",
    "                    target_actor(replay_batch['next_state']) + noise, min=min_action, max=max_action)\n",
    "\n",
    "                # Compute target Q value$\n",
    "                target_Q1 = target_critic_1(replay_batch['next_state'], target_action)\n",
    "                target_Q2 = target_critic_2(replay_batch['next_state'], target_action)\n",
    "                target_Q = torch.minimum(target_Q1, target_Q2)\n",
    "                y = replay_batch['reward'] + DISCOUNT*(1-replay_batch['done'])*target_Q\n",
    "\n",
    "            # Compute critic loss\n",
    "            Q1 = critic_1(replay_batch['state'], replay_batch['action'])\n",
    "            Q2 = critic_2(replay_batch['state'], replay_batch['action'])\n",
    "            critic_loss = F.mse_loss(Q1, y) + F.mse_loss(Q2, y)\n",
    "\n",
    "            # Update critics\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if step % (POLICY_FREQ) == 0:\n",
    "                # gradient ?\n",
    "                actor_loss = -critic_1(\n",
    "                    replay_batch['state'], actor(replay_batch['state'])).mean()\n",
    "\n",
    "                # Update actor\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Update target networks\n",
    "                for param, target_param in zip(critic_1.parameters(), target_critic_1.parameters()):\n",
    "                    target_param.data.copy_(TAU*param.data + (1-TAU)*target_param.data)\n",
    "\n",
    "                for param, target_param in zip(critic_2.parameters(), target_critic_2.parameters()):\n",
    "                    target_param.data.copy_(TAU*param.data + (1-TAU)*target_param.data)\n",
    "\n",
    "                for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                    target_param.data.copy_(TAU*param.data + (1-TAU)*target_param.data)\n",
    "            \n",
    "\n",
    "envs = [\"Acrobot-v1\", \"CartPole-v1\", \"MountainCarContinuous-v0\", \"MountainCar-v0\", \"Pendulum-v1\"]\n",
    "for env_name in envs:\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    print(env_name)\n",
    "\n",
    "    train_TD3(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
